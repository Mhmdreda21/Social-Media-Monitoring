# -*- coding: utf-8 -*-
"""FINAL PROJECT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jGPF4i66Fnd4S_01R-4QdHpWXkujo1X_
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import string, re
import itertools
import nltk
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Dense, Embedding, LSTM
from tensorflow.keras.callbacks import EarlyStopping
# %matplotlib inline

# from google.colab import drive
# drive.mount('/content/drive')

data=pd.read_table('./all_dataset.txt',
                   header=None,
                   names=['Sentance','Sentiment'])
df = pd.concat([data])
df.head()

import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords
nltk.download('punkt')
nltk.download('stopwords')
import nest_asyncio
nest_asyncio.apply()

def remove_punct(x):
    comp = re.compile("[%s\d]" % re.escape(string.punctuation))
    return " ".join(comp.sub(" ", str(x)).split()).lower()
df['Sentance'] = df['Sentance'].apply(remove_punct)

max_features = 2000
tokenizer = Tokenizer(num_words=max_features, split=' ')
tokenizer.fit_on_texts(df['Sentance'].values)
X = tokenizer.texts_to_sequences(df['Sentance'].values)
X = pad_sequences(X)

embed_dim = 64
lstm_out = 16
model = Sequential()
model.add(Embedding(max_features, embed_dim, input_length=X.shape[1]))
model.add(LSTM(lstm_out))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
print(model.summary())

Y = df['Sentiment'].values

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

batch_size = 64
history=model.fit(X_train,
          Y_train,
          epochs=10,
          batch_size=batch_size,
          validation_data=(X_test, Y_test),
          callbacks = [EarlyStopping(monitor='val_accuracy',
                       min_delta=0,
                       patience=2,
                       restore_best_weights=True,
                       verbose=1)]
           )

# Commented out IPython magic to ensure Python compatibility.
import numpy as np 
# %matplotlib inline 
"""
his_def=pd.DataFrame(history.history)
his_def.plot(y=['loss','val_loss'],figsize=(12,8))
plt.grid(axis='both')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.title('model performance')
plt.xticks(np.arange(0,50,50))
plt.show()
"""

predictions_nn_train = model.predict(X_train)
predictions_nn_test = model.predict(X_test)
for i in range(len(predictions_nn_train)):
    if predictions_nn_train[i][0] < 0.5:
        predictions_nn_train[i][0] = 0
    else:
        predictions_nn_train[i][0] = 1
        
for i in range(len(predictions_nn_test)):
    if predictions_nn_test[i][0] < 0.5:
        predictions_nn_test[i][0] = 0
    else:
        predictions_nn_test[i][0] = 1
print('Train accuracy:', accuracy_score(Y_train, predictions_nn_train))
print('Test accuracy', accuracy_score(Y_test, predictions_nn_test))
print(type(Y_test))
print(Y_test.sum()/len(Y_test))

dictionary = tokenizer.word_index
sentences = []
for j in range(len(X_test)):
    sentence = []
    for i in range(len(X_test[j])):
        if X_test[j][i] == 0:
            continue
        else:
            for key, val in dictionary.items():
                if dictionary[key] == X_test[j][i]:
                    sentence.append(key)
    sentences.append(sentence)
    
err_analysis = pd.DataFrame({'Sentences': sentences,
                             'y_true': Y_test,
                             'y_pred': predictions_nn_test.reshape(1649,)})
err_analysis.head(10)
errors = err_analysis.loc[err_analysis['y_pred']!=err_analysis['y_true']]
errors.head()

df_pos = df[ df['Sentiment'] == 1]
df_neg = df[ df['Sentiment'] == 0]
all_count_pos = len(df_pos)
all_count_neg = len(df_neg)
err_count_pos = len(errors[ errors['y_true'] == 1])
err_count_neg = len(errors[ errors['y_true'] == 0])
all_count_postive_by_persentage=(all_count_pos/8244)*100
all_count_negative_by_persentage=(all_count_neg/8244)*100
table=pd.DataFrame({'Count positives: ':[all_count_pos],
                    'Count negatives: ':[all_count_neg],
                    'Errors in true positive: ':[err_count_pos],
                    'Errors in true negative: ':[err_count_neg],
                    'Fraction of the errors with true positive:':[round(err_count_pos/all_count_pos, 4)],
                    'Fraction of the errors with true negative:':[round(err_count_neg/all_count_neg, 4)],
                    'count postive_by_persentage:':[all_count_postive_by_persentage],
                    'count negative_by_persentage:':[all_count_negative_by_persentage]})
table.head()

table.to_csv('out_put8.csv')

"""  """
# !cp out_put8.csv '/content/drive/MyDrive/New_folde_78'

from tensorflow.keras.callbacks import ModelCheckpoint 
import tensorflow as tf 
checkpoint=tf.keras.callbacks.ModelCheckpoint(
    '/content/drive/MyDrive/save_model/my-model',
    monitor='val_accuracy',
    verbose=1,
    save_best_only=True,
    save_weights_only=True,
    mode='max'
 )

history=model.fit(
    X_train,Y_train,
    validation_data=(X_test,Y_test),
    epochs=10,
    batch_size=batch_size,
    callbacks=[checkpoint])

model.load_weights('/content/drive/MyDrive/folder_model/my-model')
test_loss,test_accuracy=model.evaluate(X_test,Y_test)

